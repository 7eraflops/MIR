{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f499dd18",
   "metadata": {
    "id": "f499dd18"
   },
   "source": [
    "# Open-Unmix: Audio Source Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5df68e",
   "metadata": {
    "collapsed": true,
    "id": "8d5df68e"
   },
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# np.float_ = np.float32  # musdb, museval\n",
    "\n",
    "import musdb\n",
    "import museval\n",
    "import torch, torchaudio\n",
    "\n",
    "from openunmix.predict import separate\n",
    "\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "from demucs.audio import convert_audio\n",
    "\n",
    "from asteroid.models import XUMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cyppe3g-wIo",
   "metadata": {
    "id": "5cyppe3g-wIo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "AMD Radeon RX 9070 XT\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "NfyHbEegcZiI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfyHbEegcZiI",
    "outputId": "51e8e5f8-a07f-4cc3-aedb-a523d8ec32a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " estimates  'music_separator copy 2.ipynb'   requirements.txt\n",
      " musdb18    'music_separator copy.ipynb'     temp_estimates\n",
      " musdb18hq   output\t\t\t     temp_output\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "path_to_folder = \".\"\n",
    "musdb_root = os.path.join(path_to_folder, \"musdb18\")\n",
    "estimates_base_path = os.path.join(path_to_folder, \"temp_estimates\")\n",
    "output_base_path = os.path.join(path_to_folder, \"temp_output\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(estimates_base_path, exist_ok=True)\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "es51qVJ-0h5P",
   "metadata": {
    "id": "es51qVJ-0h5P"
   },
   "outputs": [],
   "source": [
    "# Load MUSDB dataset\n",
    "mus = musdb.DB(\n",
    "    root=musdb_root,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0hRslDB_-3kY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hRslDB_-3kY",
    "outputId": "3849859f-7784-465a-9c34-80f2e2034c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[→] Separating: A Classic Education - NightOwl\n",
      "vocals          ==> SDR:   3.938  SIR:   5.421  ISR:  10.877  SAR:   6.708  \n",
      "accompaniment   ==> SDR:  12.263  SIR:  18.199  ISR:  15.721  SAR:  14.834  \n",
      "\n",
      "[→] Separating: ANiMAL - Clinic A\n",
      "vocals          ==> SDR:   3.938  SIR:   5.421  ISR:  10.877  SAR:   6.708  \n",
      "accompaniment   ==> SDR:  12.263  SIR:  18.199  ISR:  15.721  SAR:  14.834  \n",
      "\n",
      "[→] Separating: ANiMAL - Clinic A\n",
      "vocals          ==> SDR:   5.252  SIR:   8.576  ISR:  14.217  SAR:   7.013  \n",
      "accompaniment   ==> SDR:  13.004  SIR:  22.343  ISR:  17.363  SAR:  14.997  \n",
      "\n",
      "[→] Separating: ANiMAL - Easy Tiger\n",
      "vocals          ==> SDR:   5.252  SIR:   8.576  ISR:  14.217  SAR:   7.013  \n",
      "accompaniment   ==> SDR:  13.004  SIR:  22.343  ISR:  17.363  SAR:  14.997  \n",
      "\n",
      "[→] Separating: ANiMAL - Easy Tiger\n",
      "vocals          ==> SDR:   6.201  SIR:  12.574  ISR:  11.155  SAR:   6.372  \n",
      "accompaniment   ==> SDR:  14.227  SIR:  20.120  ISR:  22.557  SAR:  15.535  \n",
      "\n",
      "[→] Separating: ANiMAL - Rockshow\n",
      "vocals          ==> SDR:   6.201  SIR:  12.574  ISR:  11.155  SAR:   6.372  \n",
      "accompaniment   ==> SDR:  14.227  SIR:  20.120  ISR:  22.557  SAR:  15.535  \n",
      "\n",
      "[→] Separating: ANiMAL - Rockshow\n",
      "vocals          ==> SDR:   4.100  SIR:   6.684  ISR:  16.811  SAR:   6.979  \n",
      "accompaniment   ==> SDR:  10.070  SIR:  20.800  ISR:  13.238  SAR:  11.581  \n",
      "\n",
      "[→] Separating: Actions - Devil's Words\n",
      "vocals          ==> SDR:   4.100  SIR:   6.684  ISR:  16.811  SAR:   6.979  \n",
      "accompaniment   ==> SDR:  10.070  SIR:  20.800  ISR:  13.238  SAR:  11.581  \n",
      "\n",
      "[→] Separating: Actions - Devil's Words\n",
      "vocals          ==> SDR:   8.947  SIR:  12.121  ISR:  18.633  SAR:  10.098  \n",
      "accompaniment   ==> SDR:  10.118  SIR:  19.151  ISR:  12.868  SAR:  10.232  \n",
      "\n",
      "[→] Separating: Actions - One Minute Smile\n",
      "vocals          ==> SDR:   8.947  SIR:  12.121  ISR:  18.633  SAR:  10.098  \n",
      "accompaniment   ==> SDR:  10.118  SIR:  19.151  ISR:  12.868  SAR:  10.232  \n",
      "\n",
      "[→] Separating: Actions - One Minute Smile\n",
      "vocals          ==> SDR:   8.739  SIR:  12.426  ISR:  14.608  SAR:   8.663  \n",
      "accompaniment   ==> SDR:   9.557  SIR:  15.031  ISR:  15.129  SAR:  10.273  \n",
      "\n",
      "[→] Separating: Actions - South Of The Water\n",
      "vocals          ==> SDR:   8.739  SIR:  12.426  ISR:  14.608  SAR:   8.663  \n",
      "accompaniment   ==> SDR:   9.557  SIR:  15.031  ISR:  15.129  SAR:  10.273  \n",
      "\n",
      "[→] Separating: Actions - South Of The Water\n",
      "vocals          ==> SDR:   9.961  SIR:  13.651  ISR:  13.538  SAR:  11.566  \n",
      "accompaniment   ==> SDR:  13.632  SIR:  18.833  ISR:  19.007  SAR:  15.994  \n",
      "\n",
      "[→] Separating: Aimee Norwich - Child\n",
      "vocals          ==> SDR:   9.961  SIR:  13.651  ISR:  13.538  SAR:  11.566  \n",
      "accompaniment   ==> SDR:  13.632  SIR:  18.833  ISR:  19.007  SAR:  15.994  \n",
      "\n",
      "[→] Separating: Aimee Norwich - Child\n",
      "vocals          ==> SDR:  10.075  SIR:  12.624  ISR:  19.502  SAR:   9.856  \n",
      "accompaniment   ==> SDR:  20.580  SIR:  26.086  ISR:  24.743  SAR:  20.081  \n",
      "\n",
      "[→] Separating: Alexander Ross - Goodbye Bolero\n",
      "vocals          ==> SDR:  10.075  SIR:  12.624  ISR:  19.502  SAR:   9.856  \n",
      "accompaniment   ==> SDR:  20.580  SIR:  26.086  ISR:  24.743  SAR:  20.081  \n",
      "\n",
      "[→] Separating: Alexander Ross - Goodbye Bolero\n",
      "vocals          ==> SDR:   7.368  SIR:  11.746  ISR:  18.893  SAR:   9.436  \n",
      "accompaniment   ==> SDR:  10.872  SIR:  22.169  ISR:  15.215  SAR:  11.655  \n",
      "\n",
      "[→] Separating: Alexander Ross - Velvet Curtain\n",
      "vocals          ==> SDR:   7.368  SIR:  11.746  ISR:  18.893  SAR:   9.436  \n",
      "accompaniment   ==> SDR:  10.872  SIR:  22.169  ISR:  15.215  SAR:  11.655  \n",
      "\n",
      "[→] Separating: Alexander Ross - Velvet Curtain\n",
      "vocals          ==> SDR:   3.520  SIR:   5.030  ISR:  12.036  SAR:   6.303  \n",
      "accompaniment   ==> SDR:  10.238  SIR:  16.826  ISR:  13.187  SAR:  12.686  \n",
      "\n",
      "[→] Separating: Angela Thomas Wade - Milk Cow Blues\n",
      "vocals          ==> SDR:   3.520  SIR:   5.030  ISR:  12.036  SAR:   6.303  \n",
      "accompaniment   ==> SDR:  10.238  SIR:  16.826  ISR:  13.187  SAR:  12.686  \n",
      "\n",
      "[→] Separating: Angela Thomas Wade - Milk Cow Blues\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openunmix\"\n",
    "\n",
    "estimates_path = os.path.join(estimates_base_path, model_name)\n",
    "output_path = os.path.join(output_base_path, model_name)\n",
    "\n",
    "os.makedirs(estimates_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Iterate over all tracks in MUSDB\n",
    "for track in mus:\n",
    "    print(f\"[→] Separating: {track.name}\")\n",
    "    audio = torch.tensor(track.audio.T).float()  # shape (2, samples)\n",
    "    rate = track.rate\n",
    "    subset = track.subset\n",
    "    estimates = separate(\n",
    "        audio=audio,\n",
    "        rate=rate,\n",
    "        targets=[\"vocals\"],\n",
    "        residual=True,\n",
    "        device=device,\n",
    "    )\n",
    "    estimates[\"accompaniment\"] = estimates.pop(\"residual\")\n",
    "    cpu_estimates = {\n",
    "        key: torch.squeeze(value).detach().cpu().numpy().T  # shape (samples, 2)\n",
    "        for key, value in estimates.items()\n",
    "    }\n",
    "    scores = museval.eval_mus_track(track, cpu_estimates, output_dir=output_path)\n",
    "    print(scores)\n",
    "\n",
    "    # Create subdirectory for the subset if it doesn't exist\n",
    "    subset_path = os.path.join(estimates_path, subset)\n",
    "    os.makedirs(subset_path, exist_ok=True)\n",
    "\n",
    "    for target, audio_np in estimates.items():\n",
    "        audio_np = (\n",
    "            torch.squeeze(audio_np).detach().cpu().numpy().T\n",
    "        )  # shape (samples, 2)\n",
    "        file_name = f\"{track.name} - {target}.wav\"\n",
    "        out_path = os.path.join(subset_path, file_name)\n",
    "        sf.write(out_path, audio_np, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"htdemucs\"\n",
    "\n",
    "estimates_path = os.path.join(estimates_base_path, model_name)\n",
    "output_path = os.path.join(output_base_path, model_name)\n",
    "\n",
    "os.makedirs(estimates_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "demucs_model = get_model(\"htdemucs\")\n",
    "demucs_model.to(device)\n",
    "\n",
    "for track in mus:\n",
    "    print(f\"[→] Separating: {track.name}\")\n",
    "\n",
    "    # Prepare audio for demucs (expects shape: batch, channels, samples)\n",
    "    audio = torch.tensor(track.audio.T).float().unsqueeze(0)  # shape (1, 2, samples)\n",
    "    audio = audio.to(device)\n",
    "    rate = track.rate\n",
    "    subset = track.subset\n",
    "\n",
    "    # Convert audio to model's expected sample rate if needed\n",
    "    audio = convert_audio(\n",
    "        audio, rate, demucs_model.samplerate, demucs_model.audio_channels\n",
    "    )\n",
    "\n",
    "    # Apply separation - FIXED: Remove the extra [None] indexing\n",
    "    with torch.no_grad():\n",
    "        sources = apply_model(demucs_model, audio, device=device)\n",
    "\n",
    "    # Remove batch dimension from sources\n",
    "    sources = sources.squeeze(0)  # Now shape: (n_sources, channels, samples)\n",
    "\n",
    "    # HTDemucs returns: [drums, bass, other, vocals]\n",
    "    # Convert to our desired format\n",
    "    estimates = {}\n",
    "    source_names = demucs_model.sources\n",
    "\n",
    "    for i, source_name in enumerate(source_names):\n",
    "        if source_name == \"vocals\":\n",
    "            estimates[\"vocals\"] = sources[i]\n",
    "        elif source_name in [\"drums\", \"bass\", \"other\"]:\n",
    "            if \"accompaniment\" not in estimates:\n",
    "                estimates[\"accompaniment\"] = sources[i]\n",
    "            else:\n",
    "                estimates[\"accompaniment\"] += sources[i]\n",
    "\n",
    "    # If no vocals found, create accompaniment from all non-vocal sources\n",
    "    if \"accompaniment\" not in estimates:\n",
    "        estimates[\"accompaniment\"] = sources.sum(dim=0) - estimates.get(\"vocals\", 0)\n",
    "\n",
    "    # Convert back to original sample rate if needed\n",
    "    for key in estimates:\n",
    "        if demucs_model.samplerate != rate:\n",
    "            estimates[key] = torchaudio.functional.resample(\n",
    "                estimates[key], demucs_model.samplerate, rate\n",
    "            )\n",
    "\n",
    "    # Prepare estimates for evaluation\n",
    "    cpu_estimates = {\n",
    "        key: torch.squeeze(value).detach().cpu().numpy().T  # shape (samples, 2)\n",
    "        for key, value in estimates.items()\n",
    "    }\n",
    "\n",
    "    # Evaluate with museval\n",
    "    scores = museval.eval_mus_track(track, cpu_estimates, output_dir=output_path)\n",
    "    print(scores)\n",
    "\n",
    "    # Create subdirectory for the subset if it doesn't exist\n",
    "    subset_path = os.path.join(estimates_path, subset)\n",
    "    os.makedirs(subset_path, exist_ok=True)\n",
    "\n",
    "    # Save separated audio files\n",
    "    for target, audio_tensor in estimates.items():\n",
    "        audio_np = (\n",
    "            torch.squeeze(audio_tensor).detach().cpu().numpy().T\n",
    "        )  # shape (samples, 2)\n",
    "        file_name = f\"{track.name} - {target}.wav\"\n",
    "        out_path = os.path.join(subset_path, file_name)\n",
    "        sf.write(out_path, audio_np, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4656a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the X-UMX model\n",
    "x_umx_model = XUMX.from_pretrained(\"JorisCos/asteroid-xumx\")\n",
    "x_umx_model.to(device)\n",
    "\n",
    "model_name = \"x-umx-asteroid\"\n",
    "\n",
    "estimates_path = os.path.join(estimates_base_path, model_name)\n",
    "output_path = os.path.join(output_base_path, model_name)\n",
    "\n",
    "os.makedirs(estimates_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for track in mus:\n",
    "    print(f\"[→] Separating: {track.name}\")\n",
    "    audio = torch.tensor(track.audio.T).float().unsqueeze(0)  # shape (1, 2, samples)\n",
    "    audio = audio.to(device)\n",
    "    rate = track.rate\n",
    "    subset = track.subset\n",
    "\n",
    "    # Perform separation using X-UMX from Asteroid\n",
    "    with torch.no_grad():\n",
    "        estimates = x_umx_model.separate(audio)\n",
    "\n",
    "    # Convert estimates to CPU and numpy format\n",
    "    cpu_estimates = {\n",
    "        key: torch.squeeze(value).detach().cpu().numpy().T  # shape (samples, 2)\n",
    "        for key, value in estimates.items()\n",
    "    }\n",
    "\n",
    "    # Evaluate with museval\n",
    "    scores = museval.eval_mus_track(track, cpu_estimates, output_dir=output_path)\n",
    "    print(scores)\n",
    "\n",
    "    # Create subdirectory for the subset if it doesn't exist\n",
    "    subset_path = os.path.join(estimates_path, subset)\n",
    "    os.makedirs(subset_path, exist_ok=True)\n",
    "\n",
    "    # Save separated audio files\n",
    "    for target, audio_tensor in estimates.items():\n",
    "        audio_np = (\n",
    "            torch.squeeze(audio_tensor).detach().cpu().numpy().T\n",
    "        )  # shape (samples, 2)\n",
    "        file_name = f\"{track.name} - {target}.wav\"\n",
    "        out_path = os.path.join(subset_path, file_name)\n",
    "        sf.write(out_path, audio_np, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e101db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_museval_results(base_path, model_names):\n",
    "    \"\"\"\n",
    "    Load museval results from JSON files for multiple models\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model_path = os.path.join(base_path, model_name)\n",
    "        results = []\n",
    "\n",
    "        # Find all JSON files in the model directory\n",
    "        json_files = glob.glob(os.path.join(model_path, \"**/*.json\"), recursive=True)\n",
    "\n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                # Extract track name from filename or data\n",
    "                track_name = os.path.basename(json_file).replace(\".json\", \"\")\n",
    "\n",
    "                # Parse the results structure\n",
    "                if \"targets\" in data:\n",
    "                    for target in data[\"targets\"]:\n",
    "                        if target[\"name\"] in [\"vocals\", \"accompaniment\"]:\n",
    "                            for frame in target[\"frames\"]:\n",
    "                                results.append(\n",
    "                                    {\n",
    "                                        \"model\": model_name,\n",
    "                                        \"track\": track_name,\n",
    "                                        \"target\": target[\"name\"],\n",
    "                                        \"sdr\": frame[\"metrics\"][\"SDR\"],\n",
    "                                        \"sir\": frame[\"metrics\"][\"SIR\"],\n",
    "                                        \"sar\": frame[\"metrics\"][\"SAR\"],\n",
    "                                        \"isr\": (\n",
    "                                            frame[\"metrics\"][\"ISR\"]\n",
    "                                            if \"ISR\" in frame[\"metrics\"]\n",
    "                                            else np.nan\n",
    "                                        ),\n",
    "                                    }\n",
    "                                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "        all_results[model_name] = results\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def create_comparison_dataframe(results_dict):\n",
    "    \"\"\"\n",
    "    Convert results dictionary to a pandas DataFrame for analysis\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        all_data.extend(results)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics for each model and target\n",
    "    \"\"\"\n",
    "    metrics = [\"sdr\", \"sir\", \"sar\", \"isr\"]\n",
    "    stats_results = {}\n",
    "\n",
    "    for model in df[\"model\"].unique():\n",
    "        stats_results[model] = {}\n",
    "        model_data = df[df[\"model\"] == model]\n",
    "\n",
    "        for target in model_data[\"target\"].unique():\n",
    "            target_data = model_data[model_data[\"target\"] == target]\n",
    "            stats_results[model][target] = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                if (\n",
    "                    metric in target_data.columns\n",
    "                    and not target_data[metric].isna().all()\n",
    "                ):\n",
    "                    values = target_data[metric].dropna()\n",
    "                    stats_results[model][target][metric] = {\n",
    "                        \"mean\": values.mean(),\n",
    "                        \"median\": values.median(),\n",
    "                        \"std\": values.std(),\n",
    "                        \"min\": values.min(),\n",
    "                        \"max\": values.max(),\n",
    "                        \"q25\": values.quantile(0.25),\n",
    "                        \"q75\": values.quantile(0.75),\n",
    "                        \"count\": len(values),\n",
    "                    }\n",
    "\n",
    "    return stats_results\n",
    "\n",
    "\n",
    "def plot_comparison_boxplots(df, figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    Create box plots comparing metrics across models\n",
    "    \"\"\"\n",
    "    metrics = [\"sdr\", \"sir\", \"sar\", \"isr\"]\n",
    "    targets = df[\"target\"].unique()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in df.columns:\n",
    "            sns.boxplot(data=df, x=\"target\", y=metric, hue=\"model\", ax=axes[i])\n",
    "            axes[i].set_title(f\"{metric.upper()} Comparison\")\n",
    "            axes[i].set_ylabel(f\"{metric.upper()} (dB)\")\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].legend(title=\"Model\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(df, figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Create distribution plots for each metric\n",
    "    \"\"\"\n",
    "    metrics = [\"sdr\", \"sir\", \"sar\", \"isr\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in df.columns:\n",
    "            for target in df[\"target\"].unique():\n",
    "                for model in df[\"model\"].unique():\n",
    "                    data = df[(df[\"target\"] == target) & (df[\"model\"] == model)][\n",
    "                        metric\n",
    "                    ].dropna()\n",
    "                    if len(data) > 0:\n",
    "                        axes[i].hist(\n",
    "                            data, alpha=0.6, label=f\"{model} - {target}\", bins=20\n",
    "                        )\n",
    "\n",
    "            axes[i].set_title(f\"{metric.upper()} Distribution\")\n",
    "            axes[i].set_xlabel(f\"{metric.upper()} (dB)\")\n",
    "            axes[i].set_ylabel(\"Frequency\")\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_per_track_comparison(df, metric=\"sdr\", figsize=(16, 8)):\n",
    "    \"\"\"\n",
    "    Create per-track comparison plots\n",
    "    \"\"\"\n",
    "    # Calculate mean metric per track for each model\n",
    "    track_means = (\n",
    "        df.groupby([\"track\", \"model\", \"target\"])[metric]\n",
    "        .mean()\n",
    "        .unstack([\"model\", \"target\"])\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    # Vocals comparison\n",
    "    if (\"openunmix\", \"vocals\") in track_means.columns and (\n",
    "        \"htdemucs\",\n",
    "        \"vocals\",\n",
    "    ) in track_means.columns:\n",
    "        axes[0].scatter(\n",
    "            track_means[(\"openunmix\", \"vocals\")],\n",
    "            track_means[(\"htdemucs\", \"vocals\")],\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        axes[0].plot(\n",
    "            [track_means.min().min(), track_means.max().max()],\n",
    "            [track_means.min().min(), track_means.max().max()],\n",
    "            \"r--\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        axes[0].set_xlabel(f\"OpenUnmix Vocals {metric.upper()} (dB)\")\n",
    "        axes[0].set_ylabel(f\"HTDemucs Vocals {metric.upper()} (dB)\")\n",
    "        axes[0].set_title(f\"Per-Track Vocals {metric.upper()} Comparison\")\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accompaniment comparison\n",
    "    if (\"openunmix\", \"accompaniment\") in track_means.columns and (\n",
    "        \"htdemucs\",\n",
    "        \"accompaniment\",\n",
    "    ) in track_means.columns:\n",
    "        axes[1].scatter(\n",
    "            track_means[(\"openunmix\", \"accompaniment\")],\n",
    "            track_means[(\"htdemucs\", \"accompaniment\")],\n",
    "            alpha=0.7,\n",
    "            color=\"orange\",\n",
    "        )\n",
    "        axes[1].plot(\n",
    "            [track_means.min().min(), track_means.max().max()],\n",
    "            [track_means.min().min(), track_means.max().max()],\n",
    "            \"r--\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        axes[1].set_xlabel(f\"OpenUnmix Accompaniment {metric.upper()} (dB)\")\n",
    "        axes[1].set_ylabel(f\"HTDemucs Accompaniment {metric.upper()} (dB)\")\n",
    "        axes[1].set_title(f\"Per-Track Accompaniment {metric.upper()} Comparison\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def statistical_significance_test(df):\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests between models\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    metrics = [\"sdr\", \"sir\", \"sar\", \"isr\"]\n",
    "    models = df[\"model\"].unique()\n",
    "\n",
    "    if len(models) == 2:\n",
    "        model1, model2 = models\n",
    "\n",
    "        for target in df[\"target\"].unique():\n",
    "            results[target] = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                if metric in df.columns:\n",
    "                    data1 = df[(df[\"model\"] == model1) & (df[\"target\"] == target)][\n",
    "                        metric\n",
    "                    ].dropna()\n",
    "                    data2 = df[(df[\"model\"] == model2) & (df[\"target\"] == target)][\n",
    "                        metric\n",
    "                    ].dropna()\n",
    "\n",
    "                    if len(data1) > 0 and len(data2) > 0:\n",
    "                        # Perform Wilcoxon rank-sum test (Mann-Whitney U test)\n",
    "                        statistic, p_value = stats.mannwhitneyu(\n",
    "                            data1, data2, alternative=\"two-sided\"\n",
    "                        )\n",
    "\n",
    "                        # Effect size (Cohen's d approximation)\n",
    "                        pooled_std = np.sqrt(\n",
    "                            (\n",
    "                                (len(data1) - 1) * data1.var()\n",
    "                                + (len(data2) - 1) * data2.var()\n",
    "                            )\n",
    "                            / (len(data1) + len(data2) - 2)\n",
    "                        )\n",
    "                        effect_size = (\n",
    "                            (data1.mean() - data2.mean()) / pooled_std\n",
    "                            if pooled_std > 0\n",
    "                            else 0\n",
    "                        )\n",
    "\n",
    "                        results[target][metric] = {\n",
    "                            \"statistic\": statistic,\n",
    "                            \"p_value\": p_value,\n",
    "                            \"significant\": p_value < 0.05,\n",
    "                            \"effect_size\": effect_size,\n",
    "                            f\"{model1}_mean\": data1.mean(),\n",
    "                            f\"{model2}_mean\": data2.mean(),\n",
    "                            f\"{model1}_median\": data1.median(),\n",
    "                            f\"{model2}_median\": data2.median(),\n",
    "                        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_comparison_report(output_base_path, model_names=[\"openunmix\", \"htdemucs\"]):\n",
    "    \"\"\"\n",
    "    Main function to generate comprehensive comparison report\n",
    "    \"\"\"\n",
    "    print(\"Loading museval results...\")\n",
    "    results_dict = load_museval_results(output_base_path, model_names)\n",
    "\n",
    "    if not any(results_dict.values()):\n",
    "        print(\n",
    "            \"No results found. Please check your paths and ensure museval has been run.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    df = create_comparison_dataframe(results_dict)\n",
    "    print(f\"Loaded {len(df)} data points from {len(df['track'].unique())} tracks\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    print(\"\\nCalculating statistics...\")\n",
    "    stats_results = calculate_statistics(df)\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for model in stats_results:\n",
    "        print(f\"\\n{model.upper()} Results:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for target in stats_results[model]:\n",
    "            print(f\"\\n  {target.capitalize()}:\")\n",
    "            for metric in [\"sdr\", \"sir\", \"sar\", \"isr\"]:\n",
    "                if metric in stats_results[model][target]:\n",
    "                    data = stats_results[model][target][metric]\n",
    "                    print(\n",
    "                        f\"    {metric.upper()}: {data['mean']:.2f} ± {data['std']:.2f} dB \"\n",
    "                        f\"(median: {data['median']:.2f}, n={data['count']})\"\n",
    "                    )\n",
    "\n",
    "    # Statistical significance tests\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sig_results = statistical_significance_test(df)\n",
    "\n",
    "    for target in sig_results:\n",
    "        print(f\"\\n{target.capitalize()} Comparison:\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for metric in sig_results[target]:\n",
    "            data = sig_results[target][metric]\n",
    "            significance = (\n",
    "                \"***\"\n",
    "                if data[\"p_value\"] < 0.001\n",
    "                else (\n",
    "                    \"**\"\n",
    "                    if data[\"p_value\"] < 0.01\n",
    "                    else \"*\" if data[\"p_value\"] < 0.05 else \"ns\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model1_name = [k for k in data.keys() if k.endswith(\"_mean\")][0].replace(\n",
    "                \"_mean\", \"\"\n",
    "            )\n",
    "            model2_name = [k for k in data.keys() if k.endswith(\"_mean\")][1].replace(\n",
    "                \"_mean\", \"\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"  {metric.upper()}: {model1_name}={data[f'{model1_name}_mean']:.2f} vs \"\n",
    "                f\"{model2_name}={data[f'{model2_name}_mean']:.2f} dB, \"\n",
    "                f\"p={data['p_value']:.4f} {significance}, effect_size={data['effect_size']:.2f}\"\n",
    "            )\n",
    "\n",
    "    # Generate plots\n",
    "    print(\"\\nGenerating plots...\")\n",
    "\n",
    "    # Box plots\n",
    "    fig1 = plot_comparison_boxplots(df)\n",
    "    plt.suptitle(\"Model Comparison - Box Plots\", y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Distribution plots\n",
    "    fig2 = plot_distribution_comparison(df)\n",
    "    plt.suptitle(\"Model Comparison - Distributions\", y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Per-track comparisons for each metric\n",
    "    for metric in [\"sdr\", \"sir\", \"sar\"]:\n",
    "        if metric in df.columns:\n",
    "            fig3 = plot_per_track_comparison(df, metric=metric)\n",
    "            plt.suptitle(f\"Per-Track {metric.upper()} Comparison\", y=1.02, fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "    print(\"\\nComparison analysis complete!\")\n",
    "    return df, stats_results, sig_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, stats_results, significance_results = generate_comparison_report(\n",
    "    output_base_path=output_base_path, model_names=[\"openunmix\", \"htdemucs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31229ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asteroid.models import XUMX\n",
    "\n",
    "# Initialize the X-UMX model\n",
    "x_umx_model = XUMX.from_pretrained(\"JorisCos/asteroid-xumx\")\n",
    "x_umx_model.to(device)\n",
    "\n",
    "model_name = \"x-umx-asteroid\"\n",
    "\n",
    "estimates_path = os.path.join(estimates_base_path, model_name)\n",
    "output_path = os.path.join(output_base_path, model_name)\n",
    "\n",
    "os.makedirs(estimates_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for track in mus:\n",
    "    print(f\"[→] Separating: {track.name}\")\n",
    "    audio = torch.tensor(track.audio.T).float().unsqueeze(0)  # shape (1, 2, samples)\n",
    "    audio = audio.to(device)\n",
    "    rate = track.rate\n",
    "    subset = track.subset\n",
    "\n",
    "    # Perform separation using X-UMX from Asteroid\n",
    "    with torch.no_grad():\n",
    "        estimates = x_umx_model.separate(audio)\n",
    "\n",
    "    # Convert estimates to CPU and numpy format\n",
    "    cpu_estimates = {\n",
    "        key: torch.squeeze(value).detach().cpu().numpy().T  # shape (samples, 2)\n",
    "        for key, value in estimates.items()\n",
    "    }\n",
    "\n",
    "    # Evaluate with museval\n",
    "    scores = museval.eval_mus_track(track, cpu_estimates, output_dir=output_path)\n",
    "    print(scores)\n",
    "\n",
    "    # Create subdirectory for the subset if it doesn't exist\n",
    "    subset_path = os.path.join(estimates_path, subset)\n",
    "    os.makedirs(subset_path, exist_ok=True)\n",
    "\n",
    "    # Save separated audio files\n",
    "    for target, audio_tensor in estimates.items():\n",
    "        audio_np = (\n",
    "            torch.squeeze(audio_tensor).detach().cpu().numpy().T\n",
    "        )  # shape (samples, 2)\n",
    "        file_name = f\"{track.name} - {target}.wav\"\n",
    "        out_path = os.path.join(subset_path, file_name)\n",
    "        sf.write(out_path, audio_np, rate)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MIR_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
